{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_file = \"./profile_server.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(benchmark_file)\n",
    "num_rows, num_cols = df.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_cols}\")\n",
    "print(\"Header:\", df.columns)\n",
    "\n",
    "qps = \"QPS\"\n",
    "throughput = \"throughput(token/s)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 32th row\n",
    "df.iloc[31]\n",
    "\n",
    "# the rows from the 11th to the 20th\n",
    "df.iloc[10:20]\n",
    "\n",
    "# display the first 10 rows\n",
    "df.head(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values\n",
    "unique_text_length = df['text_length'].unique()\n",
    "print(\"Unique values using unique():\", unique_text_length)\n",
    "\n",
    "# Get the number of unique values\n",
    "unique_count = df['text_length'].nunique()\n",
    "print(\"Number of unique values using nunique():\", unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"text_length\"]\n",
    "#df[\"QPS\"].max()\n",
    "# Find the row with the maximum QPS\n",
    "max_qps_row = df.loc[df[qps].idxmax()]\n",
    "# Print the row with the maximum QPS\n",
    "print(\"Max QPS:\")\n",
    "print(max_qps_row)\n",
    "\n",
    "max_throughtpus_row = df.loc[df[throughput].idxmax()]\n",
    "print(\"Max throughput:\")\n",
    "print(max_throughtpus_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'QPS' column in descending order\n",
    "df_by_qps = df.sort_values(by=qps, ascending=False)\n",
    "df_by_qps.head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the throughput column in ascending order\n",
    "df_by_throughput = df.sort_values(by=throughput, ascending=False)\n",
    "df_by_throughput.head(n=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame using multiple conditions\n",
    "filtered_df = df[(df[qps] >= 800) & (df[throughput] >= 500000)]\n",
    "print(filtered_df.shape)\n",
    "# Print the filtered DataFrame\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# (batch_size, worker_num)\n",
    "for t, w in [(128, 31), (256, 31), (512, 31), (1024, 31)]:\n",
    "    filtered_df = df[(df[\"text_length\"] == t) & (df[\"worker_num\"] == w)]\n",
    "    plt.plot(filtered_df['batch_size'], filtered_df[qps], marker='o', linestyle='-', label=f'text_length={t},worker_num={w}')\n",
    "\n",
    "plt.title(\"Batch Size vs QPS\")\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel(qps)\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# (batch_size, worker_num)\n",
    "for t, w in [(1, 31), (4, 31), (8, 31), (12, 31), (16, 31)]:\n",
    "    filtered_df = df[(df[\"batch_size\"] == t) & (df[\"worker_num\"] == w)]\n",
    "    plt.plot(filtered_df['text_length'], filtered_df[qps], marker='o', linestyle='-', label=f'batch_size={t},worker_num={w}')\n",
    "\n",
    "plt.title(\"Text Length vs QPS\")\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel(qps)\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# (batch_size, worker_num)\n",
    "for t, w in [(128, 31), (256, 31), (512, 31), (1024, 31)]:\n",
    "    filtered_df = df[(df[\"text_length\"] == t) & (df[\"worker_num\"] == w)]\n",
    "    plt.plot(filtered_df['batch_size'], filtered_df[throughput], marker='o', linestyle='-', label=f'text_length={t},worker_num={w}')\n",
    "\n",
    "plt.title(\"Batch Size vs Throughput\")\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel(throughput)\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# (batch_size, worker_num)\n",
    "for t, w in [(1, 31), (4, 31), (8, 31), (12, 31), (16, 31)]:\n",
    "    filtered_df = df[(df[\"batch_size\"] == t) & (df[\"worker_num\"] == w)]\n",
    "    plt.plot(filtered_df['text_length'], filtered_df[throughput], marker='o', linestyle='-', label=f'batch_size={t},worker_num={w}')\n",
    "\n",
    "plt.title(\"Text Length vs Throughput\")\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel(throughput)\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file_path = \"./db_statistics.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Define the bins and labels for the aggregation\n",
    "bins = [-float('inf'), 1e6, 5e6, 1e7, 5e7, 1e8, 5e8, 1e9, 5e9, 1e10, float('inf')]\n",
    "labels = [\n",
    "    '< 1e6',\n",
    "    '< 5e6',\n",
    "    '< 1e7',\n",
    "    '< 5e7',\n",
    "    '< 1e8',\n",
    "    '< 5e8',\n",
    "    '< 1e9',\n",
    "    '< 5e9',\n",
    "    '< 1e10',\n",
    "    '>= 1e10'\n",
    "]\n",
    "\n",
    "# Create a new column 'category' based on the bins\n",
    "df['category'] = pd.cut(df['data_num'], bins=bins, labels=labels)\n",
    "\n",
    "# Aggregate the results by 'category'\n",
    "result = df['category'].value_counts(sort=False).reset_index()\n",
    "result.columns = ['Range', 'Count']\n",
    "\n",
    "result_file = 'aggregated_results.csv'\n",
    "# Save the result to a new CSV file\n",
    "result.to_csv(result_file, index=False)\n",
    "\n",
    "print(f\"Aggregated results saved to {result_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {result_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values\n",
    "unique_text_length = df['category'].unique()\n",
    "print(\"Unique values using unique():\", unique_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num_field = \"data_num\"\n",
    "total_cost_field = \"total_cost\"\n",
    "filtered_df = df[df[\"category\"] == '< 1e6']\n",
    "df_by_data_num = filtered_df.sort_values(by=data_num_field, ascending=False)\n",
    "print(df_by_data_num.shape)\n",
    "df_by_data_num.sort_values(by=total_cost_field, ascending=False).head(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AD Recall Secenario\n",
    "\n",
    "* Data Number: 350万, 64维向量\n",
    "* Model: hnsw(m=8,cef=120,sef=300), 不做量化, 1分片, 单分片12G(预留了流式更新buffer, 实际6千万), 构建用时 15min\n",
    "* Service: 1分片23副本, 机器套餐48CPU192G内存, MAX_CPU_UTIL 10% 左右, AVG_CPU_UTIL 3% 左右\n",
    "* Recall: topk 10000 左右, 单分片候选覆盖数目 20000 - 50000 左右\n",
    "* QPS: Peak 6K\n",
    "* Latency Per Shard(p50, p90, p99): 10ms, 14ms, 24ms\n",
    "\n",
    "# Million Scale\n",
    "\n",
    "* Data Number: 370万, 128维向量\n",
    "* Model: hnsw(m=20,cef=400,sef=800), int8量化, 1分片, 单分片2G, 构建用时 10min\n",
    "* Service: 部署在架构公共资源池, 机器套餐32CPU512G内存, 4副本\n",
    "* Recall: 召回率 97% 左右, topk 10 左右 (10*1), 单分片候选覆盖数目 12000 - 20000 左右\n",
    "* QPS: Peak 46K\n",
    "* Latency Per Shard(p50, p90, p99): 3ms, 3.6ms, 4ms\n",
    "\n",
    "\n",
    "# 10 Million Scale\n",
    "\n",
    "* Data Number: 650万, 200维向量\n",
    "* Model: hnsw(m=20,cef=400,sef=800), int8量化, 16分片, 单分片0.3G, 构建用时 10min\n",
    "* Service: 部署在架构公共资源池, 机器套餐32CPU512G内存, 4副本\n",
    "* Recall: 召回率 100% 左右, topk 200左右 (10*16), 单分片候选覆盖数目 100000 左右 (带了过滤条件)\n",
    "* QPS: Peak 13K\n",
    "* Latency Per Shard(p50, p90, p99): 5ms, 7.6ms, 10ms\n",
    "\n",
    "\n",
    "# 50 Million Scale\n",
    "\n",
    "* Data Number: 3千万, 128维向量\n",
    "* Model: hnsw(m=24,cef=400,sef=800), int16量化, 10分片, 单分片3G, 构建用时 10min\n",
    "* Service: 部署在架构公共资源池, 机器套餐32CPU512G内存, 4副本\n",
    "* Recall: 召回率 94% 左右, topk 10000左右 (1000*10), 单分片候选覆盖数目 15000 - 30000 左右\n",
    "* QPS: Peak 15K\n",
    "* Latency Per Shard(p50, p90, p99): 3.5ms, 6ms, 7ms\n",
    "\n",
    "\n",
    "# 100 Million Scale\n",
    "\n",
    "* Data Number: 8千万, 256维向量\n",
    "* Model: hnsw(m=20,cef=400,sef=800), int8量化, 6分片, 单分片16G, 构建用时 0.5h\n",
    "* Service: 部署在架构公共资源池, 机器套餐32CPU512G内存, 4副本\n",
    "* Recall: 召回率 92% 左右, topk 60左右 (10*60), 单分片候选覆盖数目 20000 - 26000 左右\n",
    "* QPS: Peak 57K\n",
    "* Latency Per Shard(p50, p90, p99): 4ms, 6.5ms, 8ms\n",
    "\n",
    "\n",
    "# 1000 Million Scale\n",
    "\n",
    "* Data Number: 49亿, 256维向量\n",
    "* Model: hnsw(m=20,cef=400,sef=800), int16量化, 30分片, 单分片100G, 构建用时 2h+\n",
    "* Service: 10分片6副本, 机器套餐64CPU512G内存, MAX_CPU_UTIL 36% 左右, AVG_CPU_UTIL 15% 左右\n",
    "* Recall: ~~召回率 65% 左右~~, topk 200左右 (20*10), 单分片候选覆盖数目 10000 - 20000 左右\n",
    "* QPS: Peak 300K\n",
    "* Latency Per Shard(p50, p90, p99): 10ms, 17ms, 36ms\n",
    "\n",
    "\n",
    "# 10000 Million Scale\n",
    "\n",
    "* Data Number: 95亿, 128维向量\n",
    "* Model: hnsw(m=16,cef=128,sef=100), int8量化, 200分片, 单分片61G, 构建用时 1h\n",
    "* Service: 50分片7副本, 机器套餐64CPU512G内存, MAX_CPU_UTIL 30% 左右, AVG_CPU_UTIL 10% 左右\n",
    "* Recall: 召回率 65% 左右, topk 4000左右(20*200), 单分片候选覆盖数目 4000 - 5000 左右\n",
    "* QPS: Peak 4114K\n",
    "* Latency Per Shard(p50, p90, p99): <1ms, 1.1ms, 1.6ms\n",
    "\n",
    "\n",
    "# More\n",
    "\n",
    "* Data Number: 180亿左右, 128维向量\n",
    "* Model: hnsw(m=20,cef=400,sef=800), 不使用量化, 64分片, 单分片88G, 构建用时 3h+\n",
    "* Service: 32分片3副本, 机器套餐32CPU512G内存, MAX_CPU_UTIL 100%, AVG_CPU_UTIL 25% 左右\n",
    "* Recall: 召回率 98% 左右, topk 1000左右((10-20)*64), 单分片候选覆盖数目 10000 - 17000 左右\n",
    "* QPS: Peak 74K\n",
    "* Latency Per Shard(p50, p90, p99): 5ms, 12ms, 48ms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
