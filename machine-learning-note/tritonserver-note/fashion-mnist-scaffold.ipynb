{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model with `https://github.com/kefth/fashion-mnist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "val_transforms = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "fashion_mnist_data_path = \"/root/code/fashion-mnist/data\"\n",
    "trainset = datasets.FashionMNIST(fashion_mnist_data_path, train=True, download=True, transform=train_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, shuffle=True)\n",
    "valset = datasets.FashionMNIST(fashion_mnist_data_path, train=False, transform=val_transforms)\n",
    "val_loader = torch.utils.data.DataLoader(valset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 28, 28]), torch.uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data.shape, trainset.data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28]) torch.float32\n",
      "torch.Size([1]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_loader:\n",
    "  print(X.shape, X.dtype)\n",
    "  print(y.shape, y.dtype)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = trainset.classes\n",
    "class_to_idx = trainset.class_to_idx\n",
    "idx_to_class = {v: k for k, v in trainset.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from `https://github.com/kefth/fashion-mnist`\n",
    "import torch.nn as nn\n",
    "\n",
    "class FashionSimpleNet(nn.Module):\n",
    "\n",
    "    \"\"\" Simple network\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1,32, kernel_size=3, padding=1), # 28\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 14\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # 7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 64 * 7 * 7)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionSimpleNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=3136, out_features=128, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moodel_path = \"/root/code/ml-scaffold/kefth/saved-models/FashionSimpleNet-run-1.pth.tar\"\n",
    "\n",
    "net = FashionSimpleNet()\n",
    "_ = net.load_state_dict(torch.load(moodel_path, weights_only=True)[\"state_dict\"])\n",
    "net.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(net, img_path):\n",
    "  print(\"img path:\", img_path)\n",
    "  test_img = Image.open(img_path)\n",
    "  test_img = test_img.convert(\"L\")\n",
    "  #test_img.show()\n",
    "  #test_img.mode, test_img.size, test_img.format\n",
    "  fit_img = ImageOps.fit(test_img, (28, 28))\n",
    "  fit_img.show()\n",
    "  img_tensor = train_transforms(fit_img)\n",
    "  output = net(img_tensor.unsqueeze(0))\n",
    "  #print(\"Output:\", output)\n",
    "  predicted_label = output.softmax(1).argmax(1).item()\n",
    "  print(\"Predicted label:\", predicted_label, \"Predicted class:\", idx_to_class[predicted_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img path: /root/code/fashion-mnist/test-imgs/sneakers.jpeg\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APf6x9aWK/iWzdmEQkVpdrYyAc4PqD0NWJb+RLclUDzdgOn1rhfEV34sj1qX+zdQkhtWVWVPKEgBwM4OOBnPFehzzEIfLIB9SOBXPXVzDBGXZ9pz99gVAyep9fzqEiQMW85c9yoI/UmgSDHL5Pc5zVptQ8xfQ9iZOn61DLMJR/rMn07Vj3Or2Md2bOS9hS4A5QsAR/QfzqMzWSnm8kf3Vy4/MCsu21O6XUzavIJUTPzOoy3TrjFdlpmnpffNJNKnHSPaB/LNa1voGk2zFo7CAuxyzuu9mPqScmtFVVFCooVR0AGK/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACLElEQVR4AVWSTW8TMRCGZ2yvncRJu2kbKEJBQggEd8QNbvwQfg8/gxuIE1JPSFy4oyJxoAKptBQ1DUrabnbX648Zln6E8F6smcfzWq81wKsq371cLRWs6vPHx8vy296j/+EEn5b2L/ZJ0s5b5OVVgKI+HRa9kQQ/KXvbr96IFeYO94y2fpqgms7OqqMnK7Zl2JinpDtNCSfc7abb7p9t3Hf9nuuwVMGdI29yeL+E/Csmf2O2GHXraigiC6pmS3h2gNbDgFRVxJ7RyM7DNawmAhQfUb8xXMdMKmLDl5D82YIHR2aUCiETxCYkl8uryemsJ2ptQX+fhgjSaNMBzi4nfSwX3UiCysP7tQACIvY04DYn+0qu2UKSzxsr1yBFD4q6AVHFFFk3C33s2e+v/7b9hhgMMkdyuEACRJp7JXw5m/x4LkmYVHuOwmCFEFMTtNM6OBdqKQ2fu8xgSAGPgXyU6Dr9qWMEmWFVGpOOT0krBYRWcoH6lK2PKEMh1qhp/ypQpkBwCFA3olGekRaxKxpPvG6ghfM2V0IO00EHk2+U5iYIJIyAUc0BQQi9KZKLCFqE9hmfCEQmkrJKogCuQxuNUSWWJ7viwUZHcviEH7I2JpNs20IIyrKD1y5md3LB+qayC1TYrlvQEoGtSrv9cZ7389Fos6tOxtFRULnm6NmUk/2bz25t9eTF3uGLrYfjr3e3U0jyix2mrXuj1udKuPNzwmk8NL1O3+bWXPcvzj+yg13XAyxilgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 8 Predicted class: Bag\n",
      "img path: /root/code/fashion-mnist/test-imgs/trouser.jpeg\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APf68lubgRsTgli5Jx9eTXWeDXzPdgZwUQ/qa66ivLI4TJfaih6w290+PocV0vgYZjvX7ZQD8jXXUV53okP2rX/Ew6hYZoh/wJj/AIVofDdzJpFy5bJ8xRn/AIDXaUVxfhFFOveJQR/y3/mz034ZKF0K5x/z8H+Qrtq//9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABTElEQVR4AWWSsW7DMAxESUqypCR1lu7t9/TbO/QPOnQrii5NbMuixCqITQOOJoHPdzocjQL38/5xNFjJCuT4Zu8zWhgMlghEKgAN4zJUOCMRynBJhcoKFwMABgTOeLJYMC9KhQUR3HhF77GWvS0gpt+fL8Yp1zWkKrGkcjQ8Ul/E7JU0dPQ5vlx9Auj2MGIu5+dXuRBT2ENTKfx1MVph86DsSikSI6Owc4tSAxmuyKHBms1DIJdZXB9uBapA6/PIBXsPFWh9EhQGb+YUOigEa9gNGqQJgsOpyqPSYh66aNxcm3af1kmaD4Hct9vc9CuoQzoZeJqm1t9yNihJTgA9ZO19s0AG2+Cx/SZrBxsEKUN/gxXVTS+Skz0DxMqiM60KpfhDg7Pt8DEQirvBzEmVegEBFxs0FdeNbRtAOs5tGj3oxra0FCq3xn0gq27/T9yFIIMd03MAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 8 Predicted class: Bag\n",
      "img path: /root/code/fashion-mnist/test-imgs/sandal.jpeg\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APaCAASeAK49/H2mS3r2tpJG0qEjbLuQvj0BArfXVrR544N5EjgY44ye2fWrlZWpvPcCZra5SMLGUTdKArN1Oee/TPaueSytnia41GJWgXiHIyGcjK89h/WqFhpl2mowwWV3M8TMC0MzF9gHdWPI+hz+FejHrWBq3haDUJkkQQ/LnKypuAyckgetZuqmTTphALZLiMINiTSGNFA4+XaOv+NbehWLWtn50yBLif5mUfwDstamadTHjSTBdFbacjcM4pTTa//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACQElEQVR4AVVSTW/TQBDdXa9jO22JayLSQIAkTVPR0EalSAWBuMGJSy+I/4dUceEPcKAHDrTiEDhQiQhaKpI0kaN8uImT2F4vM5uAxGq8s+M37+3s7NIWmQ+58MphAB9nOINRNf+fyKn6jaikYDJGHEKcuXJAI1TzXU/YWVwvhgIxZr3GVSBnFzQHdCkUG0HAKBudTixT0kk/P/wdrTkxpJC5LPDefCTLxT3LMN1vQ9a8f1MAiqCkRIZGuet+OH6ZzHQtYzL+7iRQsgNfJM3TRiIYXdUHG8niUqz7/UpeAKlDmPt+e/V8BirUD8y3lzt7ae5lHsTqKKJ+a/1I0wmJZvKpd+CeHFb3JyGoEibpeLnQHrSguIg8ujFMll4fdN5FcQxnYFS0/a79ZDTjQ/feWuAzQe6+2umnGNTC2UDLUb9wO6F71UStMKXQgLhoZqGFkjb73NLpEv1M20XtJBuykEa+sZ+CHkt+PtJt1uvRX4/zaSPzaWuZxVNSsQX2nmsdyzs+Y6SorwSNYYEGbCJKOegtMh1z+0g8XyUO/eFx2xoyP7q+GQMPjFslYVatIDSsM21r16fNGtvUFBHKbZGo8zWaTZ9Nw+m1sGu77couEhWTED2nN+OC7idqP7+EMvVwA/dDDG9FkkxWxoE/vjReJFtReQUqXeyJID4cPXWR1q2R4dxRp1BsjvI4pFEm9bFglcT8iSB78RKUTMls8nUH7hHV0GhzzkMu0WDCtv21f7KYg5cPiMLQLWQXsSoT1/OUP9AnE6WRmwg3AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 8 Predicted class: Bag\n",
      "img path: /root/code/fashion-mnist/test-imgs/t-shirt.jpeg\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APU/GfjEeHI47e2RJb6YbgH+7Gv94/0FeYXPirX7udpn1a6Rm42xPsUfQDitbQviLqmmzRxX8hvbQEBvMH7xR6hu/wCNew288V1bx3EDh4pFDow6EHoa4Xxn4IivprrW1vZFKQtJLGwznapwFPboOPrXlKnKitnwx4dPifUp7MXPkNHF5oYrkfeAIx9Ca9r0XSYtE0mDT4ZZZY4gcNIcnk5/Ae1M8R/8izqn/XpL/wCgmvnwdOK7j4V/8jPdf9ejf+hrXr1VtQtY77Trm0lLCOaJo2KnBwRg4rhx8NdFAx599/38X/4mtnw14Q07QNQku7SS5aR4jGRK4IxkHsB6V1Ff/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABrElEQVR4AUVSsUoDQRDd2btLzpAqIkKwE0RBexWR1AZt/Q2/xS/Q3lI0EBtFFFJrUHuNilYSEpPbGd/s3sYJe7c3b9+8mbchMbMYUsUUXJt9m3S2HfRu0pqZ/G5vNmOOSubz9ePITFMpKi5fb60FOIBvZ32XWSJjxDBPafVwSWEPvh9/VgEQUBFHxKOFI61tscanX3nABCLIJHPfJ+MS7L5kyvK4TKdFwZK9dAI4uErZOXCAGlvJxKFA1n31RS6HKgVQ/Asws9DwXMHnXkZokQFjIRzLREz1/glgvuyEKDHI6cyCc8YZdiu5jlLcdt72+h+UWIEqmEVz9WKxvZuqfWlr427nQUlYGszt+tYCNt7b+f0xi28WGagWtQM95E3Ai4UDU6APqsciiO9EE1rZ2+FRtQ+hjoKMSfWnjWlEUCCOvbfClFVnly3EqIdmUNa6QIzM8stT2SblUGVZSvROcERXUib/ywY1XDTmiv+d2JCaoCwtGPuJmrC71AGMC8MRRGTiZkJCn5HqvVUThMnPggrRvdiQbYwsqFbtMUUjzhCk5Ace6BYPMkk9SPwBsh/WFTv3AtQAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 8 Predicted class: Bag\n"
     ]
    }
   ],
   "source": [
    "test_img_path = \"/root/code/fashion-mnist/test-imgs\"\n",
    "for image in os.listdir(test_img_path):\n",
    "    img_path = os.path.join(test_img_path, image)\n",
    "    predict_img(net, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on training dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count: 60000, err_count: 1569, accuracy: 97.39%\n"
     ]
    }
   ],
   "source": [
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "print(\"test on training dataset\")\n",
    "\n",
    "total_count = 0\n",
    "err_count = 0\n",
    "for i, (X, y) in enumerate(train_loader):\n",
    "  total_count += 1\n",
    "  output = net(X)\n",
    "  y_hat = output.softmax(1).argmax(1).item()\n",
    "  y = y.item()\n",
    "  if y_hat != y:\n",
    "    #print(\"predict: {}, actual: {}\".format(idx_to_class[y_hat], idx_to_class[y]))\n",
    "    #to_pil(X.squeeze()).show()\n",
    "    err_count += 1\n",
    "\n",
    "print(\"total_count: {}, err_count: {}, accuracy: {:.2f}%\".format(total_count, err_count, (total_count-err_count)*100/total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on validation dataset\n",
      "total_count: 10000, err_count: 689, accuracy: 93.11%\n"
     ]
    }
   ],
   "source": [
    "print(\"test on validation dataset\")\n",
    "\n",
    "total_count = 0\n",
    "err_count = 0\n",
    "for i, (X, y) in enumerate(val_loader):\n",
    "  total_count += 1\n",
    "  output = net(X)\n",
    "  y_hat = output.softmax(1).argmax(1).item()\n",
    "  y = y.item()\n",
    "  if y_hat != y:\n",
    "    #print(\"predict: {}, actual: {}\".format(idx_to_class[y_hat], idx_to_class[y]))\n",
    "    #to_pil(X.squeeze()).show()\n",
    "    err_count += 1\n",
    "\n",
    "print(\"total_count: {}, err_count: {}, accuracy: {:.2f}%\".format(total_count, err_count, (total_count-err_count)*100/total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"/root/code/ml-scaffold/kefth/saved-models/model.onnx\"\n",
    "torch.onnx.export(\n",
    "    net,                      # Model to be exported\n",
    "    X,                        # Dummy input tensor\n",
    "    onnx_model_path,             # Output file path\n",
    "    export_params=True,         # Store the trained parameter weights inside the model file\n",
    "    #opset_version=11,           # ONNX version to export the model to\n",
    "    do_constant_folding=True,   # Whether to execute constant folding for optimization\n",
    "    input_names=['input'],      # Input tensor names\n",
    "    output_names=['output'],    # Output tensor names\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # Dynamic axes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been successfully converted to ONNX format.\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "# Check that the model is well-formed\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"The model has been successfully converted to ONNX format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model repository for serving\n",
    "\n",
    "'''\n",
    "# tree                   \n",
    ".\n",
    "`-- kefth-fashion-mnist\n",
    "    `-- 1\n",
    "        |-- config.pbtxt\n",
    "        |-- model.onnx\n",
    "        `-- model.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tritonclient.grpc as grpcclient\n",
    "from tritonclient.utils import np_to_triton_dtype, triton_to_np_dtype\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "server_addr = \"localhost:8001\"\n",
    "triton_client = grpcclient.InferenceServerClient(server_addr)\n",
    "\n",
    "in0 = X.numpy()\n",
    "input_tensors = [\n",
    "    grpcclient.InferInput(\"input\", in0.shape, np_to_triton_dtype(in0.dtype)).set_data_from_numpy(in0),\n",
    "]\n",
    "output_tensors = [\n",
    "    grpcclient.InferRequestedOutput(\"output\"),\n",
    "]\n",
    "model_name = \"kefth-fashion-mnist\"\n",
    "infer_rsp = triton_client.infer(model_name, inputs=input_tensors, outputs=output_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.09344   -2.4736507 -4.753368   5.8362384 -5.3823166 -3.6607287\n",
      "  -8.433782  -4.2557     0.6277533 -7.025732 ]]\n"
     ]
    }
   ],
   "source": [
    "output0 = infer_rsp.as_numpy(\"output\")\n",
    "print(output0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
