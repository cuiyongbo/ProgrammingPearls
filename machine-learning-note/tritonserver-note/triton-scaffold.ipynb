{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import necessary packages into workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# for how to install tritonclient, refer to https://github.com/triton-inference-server/client\n",
    "import tritonclient.http as httpclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* start tritonserver with [example model](https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-triton/resnet50/workspace)\n",
    "\n",
    "```bash\n",
    "# docker images\n",
    "REPOSITORY                    TAG         IMAGE ID       CREATED        SIZE\n",
    "nvcr.io/nvidia/pytorch        24.01-py3   8470a68886ff   6 months ago   22GB\n",
    "nvcr.io/nvidia/tritonserver   24.01-py3   4f00c79d1c09   6 months ago   14.7GB\n",
    "\n",
    "# docker run --gpus all -t -d --rm --ipc=host --network=host --ulimit memlock=-1 --ulimit stack=67108864 -v /home/cuiyongbo/tensorflow-workspace/data-store/huggingface-store:/model-store nvcr.io/nvidia/tritonserver:24.01-py3\n",
    "\n",
    "# docker container list\n",
    "CONTAINER ID   IMAGE                                   COMMAND                  CREATED          STATUS          PORTS     NAMES\n",
    "0269f37660a5   nvcr.io/nvidia/tritonserver:24.01-py3   \"/opt/nvidia/nvidia_…\"   12 seconds ago   Up 11 seconds             quirky_heyrovsky\n",
    "\n",
    "# docker exec -u root -it 775a5a810f15 bash\n",
    "\n",
    "# model hierarchy:\n",
    "# tree /model-store/amazon-sagemaker-examples/sagemaker-triton/resnet50/triton-serve-pt\n",
    "/model-store/amazon-sagemaker-examples/sagemaker-triton/resnet50/triton-serve-pt\n",
    "└── resnet\n",
    "    ├── 1\n",
    "    │   └── model.pt\n",
    "    └── config.pbtxt\n",
    "\n",
    "2 directories, 2 files\n",
    "\n",
    "# tritonserver --model-store=/model-store/amazon-sagemaker-examples/sagemaker-triton/resnet50/triton-serve-pt --allow-metrics=false --allow-grpc=false --model-control-mode=explicit --load-model=resnet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"localhost:8000\" # tritonserver's address\n",
    "verbose = True\n",
    "triton_client = httpclient.InferenceServerClient(\n",
    "    url=url, verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"simple_string\"\n",
    "inputs = []\n",
    "inputs.append(httpclient.InferInput(\"INPUT0\", [1, 16], \"BYTES\"))\n",
    "inputs.append(httpclient.InferInput(\"INPUT1\", [1, 16], \"BYTES\"))\n",
    "in0 = np.arange(start=0, stop=16, dtype=np.int32)\n",
    "in0 = np.expand_dims(in0, axis=0)\n",
    "in1 = np.ones(shape=(1, 16), dtype=np.int32)\n",
    "expected_sum = np.add(in0, in1)\n",
    "expected_diff = np.subtract(in0, in1)\n",
    "in0n = np.array([str(x).encode(\"utf-8\") for x in in0.reshape(in0.size)], dtype=np.object_)\n",
    "in1n = np.array([str(x).encode(\"utf-8\") for x in in1.reshape(in1.size)], dtype=np.object_)\n",
    "inputs[0].set_data_from_numpy(in0n.reshape(in0.shape), binary_data=True)\n",
    "inputs[1].set_data_from_numpy(in1n.reshape(in1.shape), binary_data=False)\n",
    "\n",
    "outputs = []\n",
    "outputs.append(httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=True))\n",
    "outputs.append(httpclient.InferRequestedOutput(\"OUTPUT1\", binary_data=True))\n",
    "\n",
    "results = triton_client.infer(model_name=model_name, inputs=inputs, outputs=outputs)\n",
    "\n",
    "output0_data = results.as_numpy(\"OUTPUT0\")\n",
    "output1_data = results.as_numpy(\"OUTPUT1\")\n",
    "\n",
    "for i in range(in0.size):\n",
    "  if expected_sum[0][i] != int(output0_data[0][i]):\n",
    "    print(\"error: incorrect sum\")\n",
    "    break\n",
    "  if expected_diff[0][i] != int(output1_data[0][i]):\n",
    "    print(\"error: incorrect difference\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(results)\n",
    "#help(results.get_output)\n",
    "#help(results.as_numpy)\n",
    "#results.get_output(\"OUTPUT0\")\n",
    "#results.as_numpy(\"OUTPUT0\")\n",
    "np.array_equal(expected_diff, output1_data)\n",
    "print(expected_diff, expected_diff.dtype)\n",
    "print(output1_data, output1_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"simple_identity\"\n",
    "\n",
    "inputs = []\n",
    "inputs.append(httpclient.InferInput(\"INPUT0\", [1, 16], \"BYTES\"))\n",
    "null_char_array = np.array([str(\"hello\").encode(\"utf-8\") for x in range(16)], dtype=np.object_)\n",
    "null_char_data = null_char_array.reshape([1, 16])\n",
    "inputs[0].set_data_from_numpy(null_char_data, binary_data=True)\n",
    "\n",
    "outputs = []\n",
    "outputs.append(httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=True))\n",
    "\n",
    "results = triton_client.infer(model_name=model_name, inputs=inputs, outputs=outputs)\n",
    "output0_data = results.as_numpy(\"OUTPUT0\")\n",
    "if not np.array_equal(null_char_data, output0_data):\n",
    "  print(\"incorrect output:\", output0_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(triton_client.is_server_live())\n",
    "print(triton_client.is_server_ready())\n",
    "print(triton_client.is_model_ready(model_name=model_name))\n",
    "meta_data = triton_client.get_server_metadata()\n",
    "pprint(meta_data)\n",
    "meta_data = triton_client.get_model_metadata(model_name=model_name)\n",
    "pprint(meta_data)\n",
    "model_repo = triton_client.get_model_repository_index()\n",
    "pprint(model_repo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BE CAUTIOUS! make sure you run the code in the same machine where tritonserver resides\n",
    "\n",
    "import tritonclient.utils.shared_memory as shm\n",
    "from tritonclient import utils\n",
    "\n",
    "#triton_client.unregister_cuda_shared_memory()\n",
    "triton_client.unregister_system_shared_memory()\n",
    "\n",
    "model_name = \"simple\"\n",
    "model_version = \"\"\n",
    "\n",
    "input0_data = np.arange(start=0, stop=16, dtype=np.int32)\n",
    "input1_data = np.ones(shape=16, dtype=np.int32)\n",
    "\n",
    "input_byte_size = input0_data.size * input0_data.itemsize\n",
    "output_byte_size = input_byte_size\n",
    "\n",
    "shm_op_handle = shm.create_shared_memory_region(\"output_data\", \"/output_simple\", output_byte_size*2)\n",
    "triton_client.register_system_shared_memory(\"output_data\", \"/output_simple\", output_byte_size*2)\n",
    "\n",
    "shm_ip_handle = shm.create_shared_memory_region(\"input_data\", \"/input_simple\", input_byte_size*2)\n",
    "shm.set_shared_memory_region(shm_ip_handle, [input0_data])\n",
    "shm.set_shared_memory_region(shm_ip_handle, [input1_data], offset=input_byte_size)\n",
    "triton_client.register_system_shared_memory(\"input_data\", \"/input_simple\", input_byte_size*2)\n",
    "\n",
    "inputs = []\n",
    "inputs.append(httpclient.InferInput(\"INPUT0\", [1, 16], \"INT32\"))\n",
    "inputs[-1].set_shared_memory(\"input_data\", input_byte_size)\n",
    "inputs.append(httpclient.InferInput(\"INPUT1\", [1, 16], \"INT32\"))\n",
    "inputs[-1].set_shared_memory(\"input_data\", input_byte_size, offset=input_byte_size)\n",
    "\n",
    "outputs = []\n",
    "outputs.append(httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=True))\n",
    "outputs[-1].set_shared_memory(\"output_data\", output_byte_size)\n",
    "outputs.append(httpclient.InferRequestedOutput(\"OUTPUT1\", binary_data=True))\n",
    "outputs[-1].set_shared_memory(\"output_data\", output_byte_size, offset=output_byte_size)\n",
    "\n",
    "results = triton_client.infer(model_name=model_name, inputs=inputs, outputs=outputs)\n",
    "\n",
    "output0 = results.get_output(\"OUTPUT0\")\n",
    "output0_data = shm.get_contents_as_numpy(shm_op_handle,\n",
    "                                         utils.triton_to_np_dtype(output0[\"datatype\"]),\n",
    "                                         output0[\"shape\"])\n",
    "\n",
    "\n",
    "output1 = results.get_output(\"OUTPUT1\")\n",
    "output1_data = shm.get_contents_as_numpy(shm_op_handle,\n",
    "                                         utils.triton_to_np_dtype(output1[\"datatype\"]),\n",
    "                                         output1[\"shape\"], offset=output_byte_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
